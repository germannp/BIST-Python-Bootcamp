{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6 Optimizing Farametric Functions, Integration, and ODEs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will cover two things today:\n",
    "\n",
    "1. Numerical integration of ODE systems\n",
    "2. Numerical optimization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First of all we include some basic packages to allow us to make nice-looking plots, manipulate data, ...:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import seaborn as sns\n",
    "sns.set()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Motivation for Numerical Integration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's say we have a system, defined as a system of differential equations\n",
    "\n",
    "$$\n",
    "\\frac{d\\mathbf{y}}{dt} = f(t, \\mathbf{y}(t))\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y}$ represents some vector of dependent variables, $t$ is time, and the system has some specified initial condition $\\mathbf{y}(t_0) = \\mathbf{y}_0 $.\n",
    "\n",
    "In a number of cases, we can find an analytical solution through direct integration or other means. \n",
    "**Eventually, finding an analytical solution becomes tricky or intractable (this is the case in most real-world ODE models, particularly nonlinear ODE systems). **\n",
    "At that point it is necessary to find a numerical approximation to the 'real' solution, which is where numerical integration comes in.\n",
    "\n",
    "We will be dealing today with the case of initial value problems, as defined above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The Euler Method\n",
    "\n",
    "In the general definition of the system above, the right hand side of the equation defines the rate of change of some quantity $\\mathbf{y}$ with respect to time.\n",
    "\n",
    "We won't go into the derivation here, but for a sufficiently small time step, the slope will not change much and so a sequence of points can be computed by iteratively computing this rate of change ($f(t,y)$) based on the value of a previous point and a time increment $h$:\n",
    "\n",
    "$$\n",
    "y_{n+1} = y_n + h(f(t_n, y_n))\n",
    "$$\n",
    "\n",
    "This is the most basic form of numerical integration, the so called Euler method.\n",
    "Let's make an example to see how it works, by integrating the function\n",
    "\n",
    "$$\n",
    "\\frac{dy}{dt} = -y(t)\n",
    "$$\n",
    "\n",
    "whose exact solution is\n",
    "\n",
    "$$\n",
    "y(t) = y_0 \\cdot e^{-t}\n",
    "$$\n",
    "\n",
    "where $y_0$ is the initial value of the system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def rhs_function(y, t):\n",
    "    '''\n",
    "    The right hand side of our ODE.\n",
    "    '''\n",
    "    dydt = -y\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def real_solution(y0, t):\n",
    "    '''\n",
    "    The real (analytical) solution to the ODE\n",
    "    '''\n",
    "    return y0*np.exp(-t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def euler_solve(t0, t1, h, y0, func):\n",
    "    '''\n",
    "    Solve a simple equation using the Euler method\n",
    "        t0: initial time\n",
    "        t1: final time\n",
    "        h: integration timestep\n",
    "        y0: initial function value\n",
    "        func: the function to integrate\n",
    "    '''\n",
    "    # Generate all timesteps\n",
    "    T = np.arange(t0, t1, h)\n",
    "    \n",
    "    # Pre-allocate space for output\n",
    "    Y = np.empty(T.shape)\n",
    "\n",
    "    # Set the initial value\n",
    "    Y[0] = y0\n",
    "\n",
    "    # Iterate over timesteps\n",
    "    for i in range(len(T) - 1):\n",
    "        # The value at the next timestep is equal to\n",
    "        # the current value, plus the rate of change multiplied by the timestep.\n",
    "        Y[i + 1] = Y[i] + h*func(Y[i], T[i])\n",
    "    \n",
    "    return T,Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "t0 = 0\n",
    "t1 = 10\n",
    "h = 0.5\n",
    "\n",
    "y0 = 1.0\n",
    "\n",
    "T, Y = euler_solve(t0, t1, h, y0, rhs_function)\n",
    "t_fine = np.linspace(T[0], T[-1], 1000)\n",
    "\n",
    "plt.plot(T, Y, 'k-o')\n",
    "plt.plot(t_fine, real_solution(y0, t_fine), 'r--')\n",
    "plt.xlabel('Time')\n",
    "plt.legend(['Euler method', 'Exact solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, the smaller the timestep becomes the closer our numerical soultion converges to the real solution.\n",
    "\n",
    "The Euler method is low order, and requires a small timestep in general in order to converge to a correct solution.\n",
    "However it is important to note is that (despite its low-order nature) it is quite useful in the case where you need to include stochastic elements in the dynamics (e.g., a noise term modelled as a Wiener process).\n",
    "\n",
    "In these cases, higher-order methods in their default formulation do not take into account additional noise terms and will generally provide an incorrect solution or flat out not work. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Solving Initial Value Problems in SciPy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Using `scipy.integrate.odeint`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In terms of ready-made numerical integration for ODEs, SciPy provides us with a few options. We will quickly take a look at two of them, imaginatively called `odeint`, shown here, and `ode`, below. These functions can be found in SciPy's `scipy.integrate` package.\n",
    "\n",
    "First, the `odeint` function ([documentation here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html)). \n",
    "\n",
    "This function essentially wraps a solver called `lsoda`, originally written in FORTRAN, in a nicer Python interface. The solver can be used to solve initial value problems of the form \n",
    "\n",
    "$$\n",
    "\\frac{d\\mathbf{y}}{dt} = f(\\mathbf{y}, ~t_0, ~\\ldots)\n",
    "$$\n",
    "\n",
    "where $\\mathbf{y}$ is our vector of dependent variables, as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import odeint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The basic form of the right hand side function is the same as before."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Solve with Euler\n",
    "t0 = 0\n",
    "t1 = 10\n",
    "h = 0.5\n",
    "\n",
    "y0 = 1.0\n",
    "\n",
    "T, Y = euler_solve(t0, t1, h, y0, rhs_function)\n",
    "\n",
    "# Solve with odeint\n",
    "sol = odeint(rhs_function, y0, T)\n",
    "\n",
    "tFine = np.linspace(T[0], T[-1], 1000)\n",
    "\n",
    "plt.plot(T, Y, 'k-o')\n",
    "plt.plot(T, sol, 'g-o')\n",
    "plt.plot(t_fine, real_solution(y0, tFine), 'r--')\n",
    "plt.xlabel('Time')\n",
    "plt.legend(['Euler method', 'odeint', 'Exact solution'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is evident that the more stable scheme used in odeint allows us to obtain a better solution using a larger timestep."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we investigate the behaviour of a more interesting system, the ['Brusselator'](https://en.wikipedia.org/wiki/Brusselator).\n",
    "\n",
    "The Brusselator is a theoretical model for a type of autocatalytic chemical reaction.\n",
    "Its behaviour is characterised by the following set of reactions:\n",
    "\n",
    "1. $ A \\rightarrow X $\n",
    "2. $ 2X + Y \\rightarrow 3X $\n",
    "3. $ B + X \\rightarrow Y + D $\n",
    "4. $ X \\rightarrow E $\n",
    "\n",
    "Thus the rate equations for the full set of reactions can be written as \n",
    "\n",
    "$$\n",
    "\\frac{d[X]}{dt} = [A] + [X]^2[Y] - [B][X] - [X]\n",
    "$$\n",
    "$$\n",
    "\\frac{d[Y]}{dt} = [B][X] - [X]^2[Y]\n",
    "$$\n",
    "\n",
    "This system is interesting, since it exhibits stable dynamics for parameters in the range $ B < 1 + A^2 $; otherwise it operates in an unstable regime where the dynamics approach a limit cycle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bruss_odeint(y, t, a, b):\n",
    "    '''\n",
    "    RHS equations for the brusselator system.\n",
    "    Arguments:\n",
    "        y: the current state of the system\n",
    "        t: time\n",
    "        a, b: free parameters governing the dynamics\n",
    "    Returns:\n",
    "        dydt: the computed derivative at (y, t)\n",
    "    '''\n",
    "    # Unpack both species\n",
    "    X, Y = y\n",
    "    # Derivatives\n",
    "    dydt = [\n",
    "        a + Y*(X**2) - (b + 1)*X, # d[X]/dt\n",
    "        b*X - Y*(X**2),           # d[Y]/dt\n",
    "    ]\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The first thing to do is to define the parameters of interest:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a = 1.0\n",
    "b = 1.0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is also necessary to define the initial conditions ($X(t_0)$ and $Y(t_0)$), and the time points over which we will integrate:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initial conditions\n",
    "y0 = [0.5, 0.5]\n",
    "# Time from 0 to 25\n",
    "t = np.linspace(0, 25, 101)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can solve the system through a single call to `odeint`.\n",
    "\n",
    "Note the function signature. `odeint()` requires the following arguments:\n",
    "- The function we wish to integrate\n",
    "- The initial point(s); these must have the same dimension as the derivative i.e., you need to have initial conditions specified for all dependent variables\n",
    "- The time over which you wish to integrate\n",
    "- Any additional arguments (optional) which will be passed to the function we're integrating; this is how we pass in our parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "sol, info = odeint(bruss_odeint, y0, t, args = (a, b), full_output = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The variable `sol` (first returned value) contains the solution over the desired time points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('The number of time points is', len(t))\n",
    "print('The solution matrix contains', sol.shape, 'entries')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, there is a column of solutions for each variable (X and Y, in this case).\n",
    "\n",
    "So, we can plot these and observe the dynamics of our system."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.plot(t, sol[:, 0])\n",
    "plt.plot(t, sol[:, 1])\n",
    "plt.xlabel('Time (a.u.)')\n",
    "plt.ylabel('Concentration (a.u.)')\n",
    "plt.legend(['[X]', '[Y]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, you may have noticed that we return a second value called `info`. \n",
    "\n",
    "This stores a record about the numerical process underlying your results. This information can be useful in case you need to debug some experiments, and additionally if you want to start building some routines that use the integrator (e.g., you may want to adapt step sizes yourself, check if a part of the integration was successful, manually model discontinuities, ...)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Let's see what 'info' contains...:\\n\")\n",
    "print(info)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These cryptically named fields are further detailed in the [documentation](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.odeint.html), but in summary:\n",
    "\n",
    "- 'hu': vector of step sizes successfully used for each time step.\n",
    "- 'tcur' vector with the value of t reached for each time step. (will always be at least as large as the input times).\n",
    "- 'tolsf': vector of tolerance scale factors, greater than 1.0, computed when a request for too much accuracy was detected.\n",
    "- 'tsw': value of t at the time of the last method switch (given for each time step)\n",
    "- 'nst': cumulative number of time steps\n",
    "- 'nfe': cumulative number of function evaluations for each time step\n",
    "- 'nje': cumulative number of jacobian evaluations for each time step\n",
    "- 'nqu': a vector of method orders for each successful step.\n",
    "- 'imxer': index of the component of largest magnitude in the weighted local error vector (e / ewt) on an error return, -1 otherwise.\n",
    "- 'lenrw': the length of the double work array required.\n",
    "- 'leniw': the length of integer work array required.\n",
    "- 'mused': a vector of method indicators for each successful time step: 1: adams (nonstiff), 2: bdf (stiff)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "According to the condition defined previously ($ B < 1 + A^2 $) we ought to be able to destabilise the system by increasing the value of our parameter $B$.\n",
    "\n",
    "Let's choose $ B = 2.5 $ and see what happens..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Same variable as before\n",
    "b = 2.5\n",
    "# Again the solution method is the same; we are just passing in a new value of b\n",
    "sol, info = odeint(bruss_odeint, y0, t, args = (a, b), full_output = True)\n",
    "# Make the plot\n",
    "plt.figure()\n",
    "plt.plot(t, sol[:, 0])\n",
    "plt.plot(t, sol[:, 1])\n",
    "plt.xlabel('Time (a.u.)')\n",
    "plt.ylabel('Concentration (a.u.)')\n",
    "plt.legend(['[X]', '[Y]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we are simulating both $[X]$ and $[Y]$, we can quite trivially plot one against the other in a phase plane."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Steady-state behaviour, for a longer time\n",
    "b = 1.0\n",
    "t = np.linspace(0, 50, 501)\n",
    "\n",
    "# Construct a grid of initial points in the phase plane\n",
    "X, Y = np.meshgrid(np.linspace(0, 2, 5), np.linspace(0, 2, 5))\n",
    "\n",
    "plt.figure()\n",
    "\n",
    "for y0 in zip(X.ravel(), Y.ravel()):\n",
    "    sol, info = odeint(bruss_odeint, y0, t, args = (a, b), full_output = True)\n",
    "    # Phase plane: we plt X vs Y, rather than a timeseries\n",
    "    plt.plot(sol[:, 0], sol[:, 1], 'k--')\n",
    "    # The initial point of the trajectory is a circle\n",
    "    plt.plot(sol[0, 0], sol[0, 1], 'ko')\n",
    "    \n",
    "plt.xlim([0, 3])\n",
    "plt.ylim([0, 2.5])\n",
    "\n",
    "plt.xlabel('[X]')\n",
    "plt.ylabel('[Y]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we're interested in seeing approximately when the transition from a stable steady state to a limit cycle occurs, we might instead plot a number of trajectories, using different values of $B$ rather than different intial conditions.\n",
    "\n",
    "_Aside: Numerical methods ('continuation') for finding the actual value of $B$ at which the transition occurs (the 'bifurcation point') are beyond the scope of this tutorial, but are discussed briefly at the end of the section. _"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Using `scipy.integrate.ode`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In many cases we will actually want more control over the numerical integration process. \n",
    "\n",
    "Helpfully, we have an alternative interface to various integration routines called simply `ode` ([documentation here](https://docs.scipy.org/doc/scipy/reference/generated/scipy.integrate.ode.html)).\n",
    "This is sold as a more object-oriented part of the module, but in fact it also provides more flexibility in terms of available solvers relative to `odeint`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.integrate import ode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**⚠ Warning ⚠** the arguments of the right-hand-side function $f(t, y)$, that is passed to `ode()`, are reversed relative to `odeint()`!\n",
    "\n",
    "I.e.,\n",
    "- ** `odeint` ** expects a RHS function of the form `func(y, t, ...)`\n",
    "- ** `ode` ** expects a RHS function of the form `func(t, y, ...)`\n",
    "\n",
    "Consequently, we define a new function for the Brusselator, where these two arguments are now in the order expected by `ode()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bruss_ode(t, y, a, b):\n",
    "    '''\n",
    "    The brusselator system, defined for scipy.integrate.ode\n",
    "    '''\n",
    "    # Unpack both species\n",
    "    X, Y = y\n",
    "    # Derivatives\n",
    "    dydt = [\n",
    "        a + Y*(X**2) - (b + 1)*X, # d[X]/dt\n",
    "        b*X - Y*(X**2),           # d[Y]/dt\n",
    "    ]\n",
    "    return dydt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Aside: We could also define this function just by swapping the arguments `t` and `y` in a wrapper function. \n",
    "This stops us from repeating code unnecessarily.\n",
    "One way of doing this is as follows:_\n",
    "\n",
    "```python\n",
    "def bruss_ode(t, y, a, b):\n",
    "    return bruss_odeint(y, t, a, b)\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generating a solution of the system is quite straightforward, it only requires us to call the function `ode.integrate()`. However obtaining the actual values of the solution can be done in multiple ways and depends a little on the solver you want to use.\n",
    "\n",
    "Here we will illustrate the use of two solvers:\n",
    "1. dopri5: higher-order solver\n",
    "2. vode: another higher-order solver that has a nice implicit method (good for stiff systems) and can take adaptively-sized time steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def make_plot(traj):\n",
    "    '''\n",
    "    A helper function to plot trajectories\n",
    "    '''\n",
    "    t = traj[:, 0]\n",
    "    y = traj[:, 1:]\n",
    "    plt.figure()\n",
    "    plt.plot(t, y[:, 0])\n",
    "    plt.plot(t, y[:, 1])\n",
    "    plt.xlabel('Time (a.u.)')\n",
    "    plt.ylabel('Concentration (a.u.)')\n",
    "    plt.legend(['[X]', '[Y]'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Dopri5 Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters as before\n",
    "a = 1.0\n",
    "b = 2.5\n",
    "\n",
    "# Define initial conditions\n",
    "y0 = [0.5, 0.5]\n",
    "t0 = 0\n",
    "\n",
    "# Final time\n",
    "t1 = 25\n",
    "\n",
    "# Define the ODE solver object\n",
    "solver = ode(bruss_ode).set_integrator('dopri5', nsteps = 10000)\n",
    "\n",
    "# System/solver parameters are set as properties on the object\n",
    "solver.set_initial_value(y0, t0).set_f_params(a, b)\n",
    "\n",
    "# Dopri5 allows for a `solout` function to be attached.\n",
    "# This function is run at the end of every complete timestep.\n",
    "# We use it here just to append solutions to the full trajectory.\n",
    "traj = []\n",
    "def solout(t, y):\n",
    "    traj.append([t, *y])\n",
    "\n",
    "solver.set_solout(solout)\n",
    "\n",
    "# Integrate until the full time\n",
    "# (Also possible to take smaller sub-steps in a loop, as in the next section)\n",
    "solver.integrate(t1)\n",
    "\n",
    "# Form an array for easier manipulation\n",
    "traj = np.asarray(traj)\n",
    "make_plot(traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Vode Solver"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters as before\n",
    "a = 1.0\n",
    "b = 10\n",
    "\n",
    "# Define initial conditions\n",
    "y0 = [0.5, 0.5]\n",
    "t0 = 0\n",
    "\n",
    "# Final time\n",
    "t1 = 100\n",
    "\n",
    "# Define the ODE solver object\n",
    "solver = ode(bruss_ode).set_integrator('vode', method = 'bdf')\n",
    "\n",
    "# System/solver parameters are set as properties on the object\n",
    "solver.set_initial_value(y0, t0).set_f_params(a, b)\n",
    "\n",
    "# We can't define a `solout` function, but we can automatically use\n",
    "# the internal automatic step of the solver\n",
    "traj = []\n",
    "while solver.successful() and solver.t < t1:\n",
    "    s = solver.integrate(t1, step = True)\n",
    "    traj.append([solver.t, *s])\n",
    "\n",
    "# Integrate until the full time\n",
    "# (Also possible to take smaller sub-steps in a loop, as in the next section)\n",
    "solver.integrate(t1)\n",
    "\n",
    "traj = np.asarray(traj)\n",
    "make_plot(traj)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some Performance Improvements: Using the Jacobian"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can specify the Jacobian, if known, to help the solver along a little bit. This will generally make the solution faster, and potentially allow the solver to perform fewer steps by providing knowledge about the system's derivatives _a priori_. \n",
    "\n",
    "More advanced packages, such as PyDSTool (mentioned below), may automatically compute a numerical or symbolic representation of the Jacobian in order to optimise simulation efficiency."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def bruss_jac(t, y, a, b):\n",
    "    '''\n",
    "    Jacobian function of the Brusselator.\n",
    "    '''\n",
    "    j = np.empty((2, 2))\n",
    "    j[0, 0] = 2*y[0]*y[1] - (b + 1)\n",
    "    j[0, 1] = y[0]**2\n",
    "    j[1, 0] = b - 2*y[0]*y[1]\n",
    "    j[1, 1] = - y[0]**2\n",
    "    return j"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Previous trajectory\n",
    "old_trajectory = traj.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define parameters as before\n",
    "a = 1.0\n",
    "b = 10.0\n",
    "\n",
    "# Define initial conditions\n",
    "y0 = [0.5, 0.5]\n",
    "t0 = 0\n",
    "\n",
    "# Final time\n",
    "t1 = 100\n",
    "\n",
    "# Define the ODE solver object\n",
    "solver = ode(bruss_ode, jac = bruss_jac).set_integrator('vode', method = 'bdf')\n",
    "\n",
    "# System/solver parameters are set as properties on the object\n",
    "solver.set_initial_value(y0, t0).set_f_params(a, b).set_jac_params(a, b)\n",
    "\n",
    "# We can't define a `solout` function, but we can automatically use\n",
    "# the internal automatic step of the solver\n",
    "traj = []\n",
    "while solver.successful() and solver.t < t1:\n",
    "    s = solver.integrate(t1, step = True)\n",
    "    traj.append([solver.t, *s])\n",
    "\n",
    "# Integrate until the full time\n",
    "# (Also possible to take smaller sub-steps in a loop, as in the next section)\n",
    "solver.integrate(t1)\n",
    "\n",
    "traj = np.asarray(traj)\n",
    "make_plot(traj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print('Original trajectory had dimensions', old_trajectory.shape)\n",
    "print('New trajectory (w/ Jacobian) has dimensions', traj.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling density of original trajectory\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.hist(old_trajectory[:, 0], bins = 50)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sampling density')\n",
    "plt.title('Sampling: no Jacobian')\n",
    "plt.ylim([0, 1200])\n",
    "\n",
    "# Sampling density of original trajectory\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.hist(traj[:, 0], bins = 50)\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('Sampling density')\n",
    "plt.title('Sampling: WITH Jacobian')\n",
    "plt.ylim([0, 1200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### An Illustration of Adaptive Sampling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One thing to bear in mind is that it's worth using an integration scheme that supports adaptive sampling of the integration step. this means that when the derivative is large, small steps are taken and _vice versa_; as a result, simulations tend to be more efficient.\n",
    "\n",
    "What does this look like in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sampling density of new trajectory, as timeseries\n",
    "plt.figure(figsize=(12, 4))\n",
    "plt.plot(traj[:, 0], traj[:, 1], '.')\n",
    "plt.xlabel('Time')\n",
    "plt.ylabel('[X]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite obvious here that the sampling is dense in the regions where changes in $[X]$ are steep, and spreads out significantly in the regions where the gradient is small."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise: Fitzhugh-Nagumo Model\n",
    "\n",
    "The Fitzhugh-Nagumo equations represent a simplified model of neuronal excitation. A particular form of the equations can be stated as follows:\n",
    "\n",
    "$$\n",
    "\\frac{dV}{dt} = -V(V - a)(V - 1) - W + I_{app}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\frac{dW}{dt} = \\epsilon (V - \\gamma W)\n",
    "$$\n",
    "\n",
    "1. Implement this system of two coupled equations as a function, and solve them for the following parameter values: $I_{app} = -0.1 $, $\\epsilon = 0.1$, $a = 0.1$, $ \\gamma = 0.25 $, under a range of initial conditions.\n",
    "2. The parameter $I_{app}$ represents an external input current or stimulation of sorts. This excites the neuron, when increased beyond a certain threshold. Find the approximate threshold value of $I_{app}$ at which the system starts continuously oscillating. What happens as you continue increasing $I_{app}$?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing Comments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally, in reality you will probably want to do some more in-depth experiments and analysis with your system (and your system may be somewhat more complex than those presented today).\n",
    "\n",
    "As an alternative to coding up your own solutions using the building blocks presented today, you could consider using the [PyDSTool](http://www.ni.gsu.edu/~rclewley/PyDSTool/FrontPage.html) package (its source code repository is [here](https://github.com/robclewley/pydstool). \n",
    "It provides quite a nice interface for modelling systems of ODEs, and additionally it does some quite clever stuff for you, saving time, effort, and testing, e.g., \n",
    "- it will compile your RHS functions in order to make simulations run faster\n",
    "- integrators support adaptive timestepping\n",
    "- automatic generation of Jacobians (and other numerical specifics) under the hood\n",
    "- automatic generation of large ODE systems (e.g., networks of coupled systems)\n",
    "and much more.\n",
    "\n",
    "PyDSTool is partially integrated with the extremely powerful numerical continuation package [AUTO](http://cmvl.cs.concordia.ca/auto/) which allows you to perform bifurcation/continuation analysis of your system directly from PyDSTool. \n",
    "\n",
    "Somewhat tangentially, if you are eventually interested in numerical continuation you'll find that modern versions of AUTO include their own Python API which allows you to build your own analysis using the full feature set of AUTO."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scipy.optimize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At some point, you may find that you need to fit a model to some data. \n",
    "\n",
    "Here we refer to a model in quite a generic sense: this could be a regression or curve fit, the outcome of an ODE or PDE model, a complex black-box system such as a multi-scale finite element simulation of heat transfer in a nuclear reactor... \n",
    "\n",
    "The important thing to consider is that the model receives a set of parameters (is parametrised) as input, and provides some data as output. \n",
    "These simulated data may be directly comparable to experimentally acquired data, and in this case we may use an optimization routine to try to estimate the parameters of our model based on the experimental data.\n",
    "\n",
    "Thus we may be able to use the model to infer hidden properties of the system, or to predict future behaviour.\n",
    "\n",
    "To this end, a baseline set of useful tools is provided by the scipy.optimize package, and we will briefly go through some of these here."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Curve Fitting\n",
    "\n",
    "Arguably the most basic optimization procedure one can perform is to fit a curve to some experimentally observed points. \n",
    "This could be a straightforward linear regression, or a more complicated nonlinear curve.\n",
    "\n",
    "This functionality is provided by the `scipy.optimize.curve_fit` function.\n",
    "Internally the function uses a nonlinear least-squares method in order to minimize the distance between some data points and a curve function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import curve_fit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to test the performance of the curve fitting provided by scipy, we will generate some synthetic data.\n",
    "This data will take the form $ A \\sin (\\omega (x - t)) $, i.e., a sinusoidal curve with amplitude $A$, frequency $\\omega$ and horizontal offset $t$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def sinus(x, A, t, omega):\n",
    "    '''\n",
    "    Sine wave, over x\n",
    "        A: the amplitude\n",
    "        omega: the frequency\n",
    "        t: the horizontal translation of the curve\n",
    "    '''\n",
    "    return A*np.sin(omega*(t + x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The range of x over which we will observe the function\n",
    "x = np.linspace(0, 2*np.pi, 100)\n",
    "\n",
    "# The 'true' data, underlying an experimental observation\n",
    "y_true = sinus(x, 1.0, 0, 2.0)\n",
    "\n",
    "# 'Measured' experimental data (what we observed); noisy\n",
    "y_measured = y_true + 0.1*np.random.randn(*y_true.shape)\n",
    "\n",
    "plt.plot(x, y_true, 'g')\n",
    "plt.plot(x, y_measured, '*k')\n",
    "plt.legend(['True data', 'Measured data'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To make our task a bit easier for this illustrative example we fix a couple of parameters and define a new function with these ($A = 1.0$ and $t = 0.0$), with only the frequency varying."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def fixed_sinus(x, p):\n",
    "    '''\n",
    "    Sinusoidal curve with all parameters fixed apart from frequency.\n",
    "        p: frequency\n",
    "    '''\n",
    "    A = 1.0\n",
    "    t = 0.0\n",
    "    return sinus(x, A, t, p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now try to fit this function to our noisy 'measured' data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# Curve fit function: fits fixedSinus to measured data over x\n",
    "popt, pcov = curve_fit(fixed_sinus, x, y_measured)\n",
    "\n",
    "# Success?\n",
    "print('*** Fitted value of wavelength was: ', popt, '\\n')\n",
    "\n",
    "plt.plot(x, y_measured, '*g')\n",
    "plt.plot(x, fixed_sinus(x, popt), '--k')\n",
    "plt.legend(['Experimental data', 'Fitted model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hmm... This is clearly quite wrong!\n",
    "It looks like although the function is quite powerful, maybe it needs a little help.\n",
    "We can sugegst an initial value for the unknown parameter to see if that helps:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Curve fit function: fits fixedSinus to measured data over x\n",
    "# Now we add the keyword argument p0, which specifies an initial guess\n",
    "popt, pcov = curve_fit(fixed_sinus, x, y_measured, p0 = 1.5)\n",
    "\n",
    "# Success?\n",
    "print('*** Fitted value of wavelength was: ', popt, '\\n')\n",
    "\n",
    "plt.plot(x, y_measured, '*g')\n",
    "plt.plot(x, fixed_sinus(x, popt), '--k')\n",
    "plt.legend(['Experimental data', 'Fitted model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Much better!\n",
    "\n",
    "Obviously this is not great as in general you may not know a good starting parameter value (although you should have an idea of bounds and constraints especially if your model is not too abstract).\n",
    "\n",
    "One way of getting around this is to initialise your fitting function from a bounded range of random initial values and choosing some that are markedly better.\n",
    "Some optimisation routines will do (basically) this internally.\n",
    "\n",
    "Another option is to choose a more robust optimisation method."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More general minimization\n",
    "\n",
    "`scipy.optimize` contains a fairly wide range of optimization routines.\n",
    "In general these aim to _minimize_ some generic function, rather than to 'fit a curve' as we did a moment ago.\n",
    "\n",
    "Therefore, in order to fit a model to some data using these methods, we must formulate an objective function.\n",
    "This function can simply compute the distance, or _error_, between our desired outcome (the measured data) and the current output of the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def objective_function(par):\n",
    "    '''\n",
    "    Compute the sum of squares difference between the measured data and model output.\n",
    "    '''\n",
    "    # The current model prediction, based on the current parameters' values\n",
    "    y_model = sinus(x, *par)\n",
    "    # Sum-of-squares difference between the model and 'measured' data\n",
    "    return np.sum(np.power(y_measured - y_model, 2.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `scipy.optimize.minimize` function provides a general interface for minimization."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from scipy.optimize import minimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We attempt to minimize our objective\n",
    "# This function now contains the full (3 parameter) sin function,\n",
    "# and so we need to provide 3 starting points; we choose [0.5, 0.5, 0.5]\n",
    "opt = minimize(objective_function, [0.5]*3)\n",
    "print('Optimal parameters: ')\n",
    "print('Amplitude: ', opt.x[0])\n",
    "print('Offset: ', opt.x[1])\n",
    "print('Frequency: ', opt.x[2])\n",
    "\n",
    "plt.plot(x, y_measured, '*g')\n",
    "plt.plot(x, sinus(x, *opt.x), '--k')\n",
    "plt.legend(['Experimental data', 'Fitted model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not bad, considering it is more generic.\n",
    "\n",
    "If we wanted further control over the process, we can also provide bounds as an optional keyword argument to the minimizer:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Here we specify that the parameters must all be positive\n",
    "bopt = minimize(objective_function, np.random.rand(3), bounds = [(0, None)]*3)\n",
    "print('Optimal parameters: ')\n",
    "print('Amplitude: ', bopt.x[0])\n",
    "print('Offset: ', bopt.x[1])\n",
    "print('Frequency: ', bopt.x[2])\n",
    "\n",
    "plt.plot(x, y_measured, '*g')\n",
    "plt.plot(x, sinus(x, *bopt.x), '--k')\n",
    "plt.legend(['Experimental data', 'Fitted model'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Closing comments\n",
    "\n",
    "This has just touched the surface of possibilities for numerical optimisation. As you can see, even the basic scipy installation provides some powerful algorithms: they may require a little tuning to get the results you need, but are in general quite robust and flexible.\n",
    "\n",
    "In particular, the routines here are designed simply to perform one function: minimize an objective function.\n",
    "Therefore, as long as you can specify an objective function appropriately, these algorithms may be useful (even if the actual computation of the objective function becomes more complicated).\n",
    "For example fitting an ODE model to some time series would basically require that you specify your objective function as the difference between an experimental time series and your model under a certain parametrisation.\n",
    "\n",
    "For working on more complex systems, or in cases where you have complicated or noisy data, other packages may present a more viable alternative. \n",
    "In reality, especially when dealing with noisy experimental data, it is crucial to choose the right optimisation routine for job. \n",
    "In many cases it may be beneficial to use a probabilistic method, for example, and often you will end up using **global optimizers**, which we didn't have time to go into today.\n",
    "\n",
    "For example, some that I like to use are:\n",
    "- [PySwarm](http://pythonhosted.org/pyswarm/): a simple constrained particle swarm optimisation tool\n",
    "- [PyGMO](http://esa.github.io/pygmo/): a very powerful serial and parallel toolbox for constructing global optimisation routines\n",
    "- [DEAP](https://github.com/DEAP/deap): another global optimisation package, strongly focused on large scale parallel evolutionary algorithms."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
